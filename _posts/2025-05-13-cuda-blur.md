---
title: GPU CUDA Average Filter
author: sebastia
date: 2025-03-08 22:28:00 +0800
categories: [C++, CUDA]
tags: [computer science, GPU]
pin: true
toc: true
render_with_liquid: false
math: true
---

This is my first post in CUDA, I have been working for a while using this technology and want to share a simple example for those who start. All the code will be available in my [github repository](https://github.com/SebastiaAgramunt/blogging-code) subdirectory [CUDA blur](https://github.com/SebastiaAgramunt/blogging-code/tree/main/cuda-blur). I would like to acqnowledge [Lambda.ai](https://cloud.lambda.ai/) for providing me with free credits for my blog. I will be testing this code with a machine `gpu_1x_a100_sxm4` which has an A100 GPU, the Ampere GPU architecture. This GPU is a bit old these days but we won't be doing any heavy compute so this will suffice.

## CUDA code

CUDA is Compute Unified Device Architecture, is a parallel computing platform and application programming interfacer (API) developed by NVIDIA. It allows developers to interact with NVIDIA GPUs for genreal purpose computing. To compile CUDA code we use the `nvcc` compiler (Nvidia CUDA Compiler). It's a specialized compiler that takes CUDA C/C++ code and compiles it into a binary that can be executed on the GPU and the host (CPU). In essence what `nvcc` does is to separate the GPU vs CPU code and compiles the former with  PTX (Parallel Thread Execution, very low level, we won't go into this) and the latter with the regular C++ compiler (`gcc`, `clang`).

The `nvcc` compiler is at a low enough level that you can write performant code, lower than this (PTX) gets way too complicated. There are libraries with already compiled code to run calculations in GPU, a good example is `pytroch`, the python package is built and linked in nvcc and the python interpreter loads the compiled library to run huge calculations on the GPU. Another very popular python package for cuda is `CuPy`, that's the equivalent of `numpy` in GPU. There are other jit (just in time compilation) libraries like `Numba`, that compile your code on the go but the API is much easier than low level GPU code. The Nvidia ecosystem has a lot of other packages, `cuDF` (for dataframes in data science), `cuML` (for machine learning), `cuGraph` (for graphs). For C++ we have compiled libraries like `cuBLAS` (basic liear algebra library), `cuSolver` (dense and sparse matrix factorizations), `cuFFT` (for fast fourier transforms), `cuRAND` (random number generation), `cuDNN` (deep neural networks).

I attended last Nvidia conference (Nvidia GTC) in San Jose and from what I saw, the company is working a lot in reducing the complexity to use GPUs by means of new python packages and easy C++ libraries. I heard a lot about `cuTile` python package in the conference but so far there is no official release.

## The mathematical problem

The idea of this post is to create a uniform filter of an image, blur the image basically but with neighboring pixels weighting the same. In [OpenCV](https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html) you can find nice documentation of a uniform filtering, a 2D convolution with same weights. In essence we want to convolve our image with the kernel


$$
Kern=\frac{1}{N^2}
\begin{bmatrix}
1 & 1 & \cdots & 1 \\
1 & 1 & \cdots & \vdots \\
\vdots & \vdots & \ddots & \vdots \\
1 & \cdots & \cdots & 1
\end{bmatrix}_{K \times K}
$$

In python we can use the opencv function `cv.filter2D` to pass a filter (kernel) to an image (check the example in the github repository). In the code we call the function with `cv.filter2D(img, -1, kernel)`, The image is the original image we read, it has `(2971, 4288, 3)` pixels, three channels for red, blue and green. The `-1` is the output channel depth that should be the same of the input and finally the kernel is the one defined above. With an $N=25$ we blur the image as shown in the picture on the right.

| Original Image | Uniform Blur $N=51$ |
|----------------|---------------------|
| <img src="/assets/img/posts/2025-05-13-cuda-blur/raw_img.jpeg" width="350px" /> | <img src="/assets/img/posts/2025-05-13-cuda-blur/blur_output_python.jpeg" width="350px" /> |

The problem of the direct convolution is that we need $O(K \times N)$ operations, where $N$ is the number of pixels of the image and $K$ the size of the kernel. For large kernels this operation may be very large and costly, remember that in CPU we compute those mutliplications sequentially (assume one thread one process for now). Normally, in CPU people use Fast Fourier Transforms (FFTs). The order for the FFT convolution is $O(N \times \log{N})$, therefore for small kernels we might be better off doing direct convolution but for large kernels the FFT may be faster. Ok, all this is for CPU, but what about GPU?. In GPU the calculations are done in parallel, a GPU is basically a massive multiprocess CPU. The faster way may be just do the direct convolution!. Although it is not free lunch, loading data to the GPU is very exepensive time-wise and launching the calculation has an overhead. So yes, direct convolution is faster in GPU but you have to consider other factors. Always benchmark your algorithms for your specific use case.

In this post we want to convolve a kernel with an image using CUDA code and then compiling with `nvcc`. Direct convolution, no FFT!.

## Project structure

The structure of my code

```bash
.
├── external
│   └── install-opencv.sh
├── include
│   ├── blur_image.cuh
│   ├── rgb_to_grayscale.cuh
│   └── utils.h
├── Makefile
├── pyproject.toml
├── README.md
├── scripts
│   ├── compile-run.sh
│   ├── download-img.sh
│   └── opencv_blur.py
├── src
│   ├── blur_image.cu
│   ├── main.cu
│   ├── rgb_to_grayscale.cu
│   └── utils.cpp
```
It contains a lot of files, yes, but stay with me, it's simpler than it looks. I have a script `install-opencv.sh` to download the binaries of opencv, compile them and place the libraries inside the project. I mentioned several times I like this pattern, at least before producion. Installing libraries system wide or vendoring is another option.  I have a `download-img.sh` file to download the source image that we will work with. Also I have a `compile-run.sh` bash script to compile "manually" the objects, we also have a `Makefile` that does the same and it is simpler. I just keep the more manual script for teaching purposes. The main code has the definitions in `blur_image.cuh`, `rgb_to_grayscale.cuh` and the implementation sin `blur_image.cu`, `rgb_to_grayscale.cu`, `utils.cpp` and the `main.cu`. The extensions `cuh` and `cu` are the cuda headers and cuda code, these are common extensions for cuda files.

## Setup

To be able to compile the code you need to first download the image, then compile and install opencv. Run the following

```bash
./scripts/download-image.sh
./external/install-opencv.sh
```

Your image will show as in `img/raw_img.jpeg`. And the libraries will be installed in `external/lib/opencv`, the headers in `include` and the libraries in `lib` (or sometimes `lib64` depending on the OpenCV version and your OS). Be patient, OpenCV compilation may take a while.

Finally to compile the code run `./scripts/compile-run.sh`. This will place a main executable in `build/bin/main`. Of course, make sure you have an Nvidia GPU in your host, also that `nvcc` is installed and that the CUDA libraries are installed (they should be in `/usr/local/cuda/include` for headers and `/usr/local/cuda/lib64` for the compiled libraries).


## The CUDA kernel

A CUDA kernel is a function written in C++ (or CUDA C++) that runs on the GPU in parallel across many threads. It is the core unit of computation in CUDA programming and is launched from the CPU (host) to be executed on the GPU (device). The kernel is declared using the `__global__` keyword indicating that it runs on th GPU and it's called from the CPU. Here is my kernel for bluring the image:

```cpp
__global__ void blur_kernel(unsigned char* input, unsigned char *output, int width, int height) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;

    if (col < width && row < height) {
        int pixVal[3] = {0, 0, 0};
        int pixels = 0;

        for(int blurRow=-BLUR_SIZE; blurRow<BLUR_SIZE+1; blurRow++){
            for(int blurCol=-BLUR_SIZE; blurCol<BLUR_SIZE+1; blurCol++){
                int curRow = row + blurRow;
                int curCol = col + blurCol;

                if (curRow >= 0 && curRow < height && curCol >= 0 && curCol < width) {
                    int offset = (curRow * width + curCol) * 3;
                    pixVal[0] += input[offset];     // R
                    pixVal[1] += input[offset + 1]; // G
                    pixVal[2] += input[offset + 2]; // B
                    pixels++;
                }
            }
        }
        int out_offset = (row * width + col) * 3;
        output[out_offset]     = static_cast<unsigned char>(pixVal[0] / pixels);
        output[out_offset + 1] = static_cast<unsigned char>(pixVal[1] / pixels);
        output[out_offset + 2] = static_cast<unsigned char>(pixVal[2] / pixels);
    }
}
```
Our image is loaded with a pointer to unsigned char, that's 8 bits (1 byte) or a number between 0 and 255 in decimal. The indexes of the pointer are row major. I.e. we stay in row first, then column and channel. In <a href="../cpp-compile-library"> cpp-compile-library</a> post we briefly mention what is row-major and column-major indices in the commented code. 

Cuda kernels are launched in blocks and grids. Basically a grid is group of blocks (see [this Nvidia blog post](https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/)). The computation structure is:

* **Threads**: are the smallest units of execution in CUDA, each thread runs the same kernel codde but on different data, theads are identified by `threadIdx` as the unique number.
* **Blocks**: A block is a group of threads. Threads in a block can share memory using the "shared memory". Each block has an index `blockIdx`.
* **Grids**: A grid is a collection of blocks. 

Blocks, grids and threads can be 1D-3D. We will go more into this later. Now see that at the begining of the funcion we define the rows and cols correspoding to the image

```cpp
int col = blockIdx.x * blockDim.x + threadIdx.x;
int row = blockIdx.y * blockDim.y + threadIdx.y;
```


